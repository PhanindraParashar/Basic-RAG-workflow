{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91a1a639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Dict, Any, List, Optional, Union, TypedDict\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.document_loaders.pdf import PyPDFDirectoryLoader \n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter \n",
    "from langchain.embeddings import OpenAIEmbeddings \n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.schema import Document \n",
    "#from langchain.vectorstores import Chroma\n",
    "from langchain_chroma import Chroma\n",
    "import datetime, json, time, os\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f53876",
   "metadata": {},
   "source": [
    "# Load Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44008120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file llama-3.pdf\n",
      "Processing file deepseek-v3.pdf\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "for filename in os.listdir(pdf_directory):\n",
    "    print(f\"Processing file {filename}\")\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        loader = PyPDFLoader(os.path.join(pdf_directory, filename))\n",
    "        documents.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8276578f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total document Pages: 145  pages\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total document Pages: {len(documents)}  pages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b0971b",
   "metadata": {},
   "source": [
    "# Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae955f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f68a497",
   "metadata": {},
   "source": [
    "# Build Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d958f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChromaDB built successfully.\n"
     ]
    }
   ],
   "source": [
    "persist_directory = \"chroma_db\"\n",
    "pdf_directory = \"Data\"  \n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "Chroma.from_documents(texts, embeddings, persist_directory=persist_directory)\n",
    "print(\"ChromaDB built successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ef53c6",
   "metadata": {},
   "source": [
    "# Define an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29c3a713",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4.1-nano\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0321a27",
   "metadata": {},
   "source": [
    "# Use the vectorstore to create a retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80acecd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
    "retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62875e54",
   "metadata": {},
   "source": [
    "# Create a QA chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "152b19c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "{context}\n",
      "{question}\n"
     ]
    }
   ],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever,return_source_documents=True)\n",
    "print(qa.combine_documents_chain.llm_chain.prompt.messages[0].prompt.template)\n",
    "print(qa.combine_documents_chain.llm_chain.prompt.messages[1].prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a35ab77",
   "metadata": {},
   "source": [
    "# Our Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af1b2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the core break-through in deep-seek v3?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e8b0ea",
   "metadata": {},
   "source": [
    "# Halucination without RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d1ea538a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As of my knowledge cutoff in October 2023, there is no publicly available information about a product or technology called \"Deep-Seek v3.\" It’s possible that it is a proprietary or emerging technology not widely documented yet, or perhaps a typo or miscommunication.\\n\\nIf you can provide more context or clarify the name or field related to \"Deep-Seek v3,\" I’d be happy to help with more specific information!'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(question).content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d21e029",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c779aca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The core breakthrough in DeepSeek-V3 is its demonstrated strong capability in handling extremely long-context tasks, validated by its top performance on LongBench v2. This long-context ability is further supported by its architecture, which includes innovations like Multi-head Latent Attention (MLA) for efficient inference and DeepSeekMoE, enabling it to process and understand lengthy inputs effectively.\n"
     ]
    }
   ],
   "source": [
    "print(qa.invoke(question)[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "662e6183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What is the core break-through in deep-seek v3?',\n",
       " 'result': \"The core breakthrough in DeepSeek-V3 is its demonstrated strong capability in handling extremely long-context tasks, validated by its top performance on LongBench v2. This long-context ability is further supported by its architecture, which includes innovations like Multi-head Latent Attention (MLA) for efficient inference and DeepSeekMoE. Additionally, DeepSeek-V3's advanced knowledge distillation techniques significantly enhance its code generation, mathematical reasoning, and problem-solving capabilities, especially in complex benchmarks. These innovations collectively contribute to its state-of-the-art performance across various domains, including long-context understanding, coding, math, and Chinese factual knowledge.\",\n",
       " 'source_documents': [Document(id='ec5249a4-4ce8-4dd4-9aad-9f7e10c33361', metadata={'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'keywords': '', 'trapped': '/False', 'subject': '', 'title': '', 'producer': 'pdfTeX-1.40.25', 'total_pages': 53, 'page_label': '32', 'creator': 'LaTeX with hyperref', 'author': '', 'source': 'Data/deepseek-v3.pdf', 'page': 31, 'creationdate': '2025-02-19T02:11:22+00:00', 'moddate': '2025-02-19T02:11:22+00:00'}, page_content='demonstrates the strong capability of DeepSeek-V3 in handling extremely long-context tasks.\\nThe long-context capability of DeepSeek-V3 is further validated by its best-in-class performance\\non LongBench v2, a dataset that was released just a few weeks before the launch of DeepSeek\\nV3. On the factual knowledge benchmark, SimpleQA, DeepSeek-V3 falls behind GPT-4o and\\nClaude-Sonnet, primarily due to its design focus and resource allocation. DeepSeek-V3 assigns\\nmore training tokens to learn Chinese knowledge, leading to exceptional performance on the\\nC-SimpleQA. On the instruction-following benchmark, DeepSeek-V3 significantly outperforms\\nits predecessor, DeepSeek-V2-series, highlighting its improved ability to understand and adhere\\nto user-defined format constraints.\\nCode and Math Benchmarks. Coding is a challenging and practical task for LLMs, encom-\\npassing engineering-focused tasks like SWE-Bench-Verified and Aider, as well as algorithmic\\ntasks such as HumanEval and LiveCodeBench. In engineering tasks, DeepSeek-V3 trails behind\\nClaude-Sonnet-3.5-1022 but significantly outperforms open-source models. The open-source\\nDeepSeek-V3 is expected to foster advancements in coding-related engineering tasks. By pro-\\nviding access to its robust capabilities, DeepSeek-V3 can drive innovation and improvement\\nin areas such as software engineering and algorithm development, empowering developers\\nand researchers to push the boundaries of what open-source models can achieve in coding'),\n",
       "  Document(id='c9d52e8a-fb5e-4c6c-b43c-e2a42be4a1b6', metadata={'total_pages': 53, 'author': '', 'source': 'Data/deepseek-v3.pdf', 'moddate': '2025-02-19T02:11:22+00:00', 'keywords': '', 'producer': 'pdfTeX-1.40.25', 'title': '', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'trapped': '/False', 'creator': 'LaTeX with hyperref', 'page': 31, 'subject': '', 'creationdate': '2025-02-19T02:11:22+00:00', 'page_label': '32'}, page_content='in areas such as software engineering and algorithm development, empowering developers\\nand researchers to push the boundaries of what open-source models can achieve in coding\\ntasks. In algorithmic tasks, DeepSeek-V3 demonstrates superior performance, outperforming\\nall baselines on benchmarks like HumanEval-Mul and LiveCodeBench. This success can be\\nattributed to its advanced knowledge distillation technique, which effectively enhances its code\\ngeneration and problem-solving capabilities in algorithm-focused tasks.\\nOn math benchmarks, DeepSeek-V3 demonstrates exceptional performance, significantly\\nsurpassing baselines and setting a new state-of-the-art for non-o1-like models. Specifically, on\\nAIME, MATH-500, and CNMO 2024, DeepSeek-V3 outperforms the second-best model, Qwen2.5\\n72B, by approximately 10% in absolute scores, which is a substantial margin for such challenging\\nbenchmarks. This remarkable capability highlights the effectiveness of the distillation technique\\nfrom DeepSeek-R1, which has been proven highly beneficial for non-o1-like models.\\nChinese Benchmarks. Qwen and DeepSeek are two representative model series with robust\\nsupport for both Chinese and English. On the factual benchmark Chinese SimpleQA, DeepSeek-\\nV3 surpasses Qwen2.5-72B by 16.4 points, despite Qwen2.5 being trained on a larger corpus\\ncompromising 18T tokens, which are 20% more than the 14.8T tokens that DeepSeek-V3 is\\n32'),\n",
       "  Document(id='c4b054b6-f8c2-4396-ac38-b0ef6ef7ddb4', metadata={'creationdate': '2025-02-19T02:11:22+00:00', 'subject': '', 'page': 5, 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'page_label': '6', 'total_pages': 53, 'moddate': '2025-02-19T02:11:22+00:00', 'source': 'Data/deepseek-v3.pdf', 'keywords': '', 'title': '', 'trapped': '/False', 'creator': 'LaTeX with hyperref', 'author': ''}, page_content='verification and reflection patterns of R1 into DeepSeek-V3 and notably improves its\\nreasoning performance. Meanwhile, we also maintain control over the output style and\\nlength of DeepSeek-V3.\\nSummary of Core Evaluation Results\\n• Knowledge: (1) On educational benchmarks such as MMLU, MMLU-Pro, and GPQA,\\nDeepSeek-V3 outperforms all other open-source models, achieving 88.5 on MMLU, 75.9\\non MMLU-Pro, and 59.1 on GPQA. Its performance is comparable to leading closed-source\\nmodels like GPT-4o and Claude-Sonnet-3.5, narrowing the gap between open-source\\nand closed-source models in this domain. (2) For factuality benchmarks, DeepSeek-V3\\ndemonstrates superior performance among open-source models on both SimpleQA and\\nChinese SimpleQA. While it trails behind GPT-4o and Claude-Sonnet-3.5 in English factual\\nknowledge (SimpleQA), it surpasses these models in Chinese factual knowledge (Chinese\\nSimpleQA), highlighting its strength in Chinese factual knowledge.\\n• Code, Math, and Reasoning: (1) DeepSeek-V3 achieves state-of-the-art performance on\\nmath-related benchmarks among all non-long-CoT open-source and closed-source models.\\nNotably, it even outperforms o1-preview on specific benchmarks, such as MATH-500,\\ndemonstrating its robust mathematical reasoning capabilities. (2) On coding-related tasks,\\nDeepSeek-V3 emerges as the top-performing model for coding competition benchmarks,\\nsuch as LiveCodeBench, solidifying its position as the leading model in this domain. For'),\n",
       "  Document(id='c88678b4-fa8e-4b88-b20d-7f4cd35b874c', metadata={'producer': 'pdfTeX-1.40.25', 'page': 5, 'source': 'Data/deepseek-v3.pdf', 'creationdate': '2025-02-19T02:11:22+00:00', 'subject': '', 'title': '', 'moddate': '2025-02-19T02:11:22+00:00', 'creator': 'LaTeX with hyperref', 'author': '', 'page_label': '6', 'keywords': '', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'total_pages': 53}, page_content='DeepSeek-V3 emerges as the top-performing model for coding competition benchmarks,\\nsuch as LiveCodeBench, solidifying its position as the leading model in this domain. For\\nengineering-related tasks, while DeepSeek-V3 performs slightly below Claude-Sonnet-3.5,\\nit still outpaces all other models by a significant margin, demonstrating its competitiveness\\nacross diverse technical benchmarks.\\nIn the remainder of this paper, we first present a detailed exposition of our DeepSeek-V3\\nmodel architecture (Section 2). Subsequently, we introduce our infrastructures, encompassing\\nour compute clusters, the training framework, the support for FP8 training, the inference\\ndeployment strategy, and our suggestions on future hardware design. Next, we describe our\\npre-training process, including the construction of training data, hyper-parameter settings, long-\\ncontext extension techniques, the associated evaluations, as well as some discussions (Section 4).\\nThereafter, we discuss our efforts on post-training, which include Supervised Fine-Tuning (SFT),\\nReinforcement Learning (RL), the corresponding evaluations, and discussions (Section 5). Lastly,\\nwe conclude this work, discuss existing limitations of DeepSeek-V3, and propose potential\\ndirections for future research (Section 6).\\n2. Architecture\\nWe first introduce the basic architecture of DeepSeek-V3, featured by Multi-head Latent Atten-\\ntion (MLA) (DeepSeek-AI, 2024c) for efficient inference and DeepSeekMoE (Dai et al., 2024)')]}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.invoke(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c97bc0",
   "metadata": {},
   "source": [
    "# Show how it works internally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8994f3",
   "metadata": {},
   "source": [
    "# Retrive the top 5 documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6c2b52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='ec5249a4-4ce8-4dd4-9aad-9f7e10c33361', metadata={'page_label': '32', 'creationdate': '2025-02-19T02:11:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': '', 'moddate': '2025-02-19T02:11:22+00:00', 'creator': 'LaTeX with hyperref', 'source': 'Data/deepseek-v3.pdf', 'keywords': '', 'subject': '', 'author': '', 'total_pages': 53, 'trapped': '/False', 'producer': 'pdfTeX-1.40.25', 'page': 31}, page_content='demonstrates the strong capability of DeepSeek-V3 in handling extremely long-context tasks.\\nThe long-context capability of DeepSeek-V3 is further validated by its best-in-class performance\\non LongBench v2, a dataset that was released just a few weeks before the launch of DeepSeek\\nV3. On the factual knowledge benchmark, SimpleQA, DeepSeek-V3 falls behind GPT-4o and\\nClaude-Sonnet, primarily due to its design focus and resource allocation. DeepSeek-V3 assigns\\nmore training tokens to learn Chinese knowledge, leading to exceptional performance on the\\nC-SimpleQA. On the instruction-following benchmark, DeepSeek-V3 significantly outperforms\\nits predecessor, DeepSeek-V2-series, highlighting its improved ability to understand and adhere\\nto user-defined format constraints.\\nCode and Math Benchmarks. Coding is a challenging and practical task for LLMs, encom-\\npassing engineering-focused tasks like SWE-Bench-Verified and Aider, as well as algorithmic\\ntasks such as HumanEval and LiveCodeBench. In engineering tasks, DeepSeek-V3 trails behind\\nClaude-Sonnet-3.5-1022 but significantly outperforms open-source models. The open-source\\nDeepSeek-V3 is expected to foster advancements in coding-related engineering tasks. By pro-\\nviding access to its robust capabilities, DeepSeek-V3 can drive innovation and improvement\\nin areas such as software engineering and algorithm development, empowering developers\\nand researchers to push the boundaries of what open-source models can achieve in coding'),\n",
       " Document(id='c9d52e8a-fb5e-4c6c-b43c-e2a42be4a1b6', metadata={'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'total_pages': 53, 'page_label': '32', 'creationdate': '2025-02-19T02:11:22+00:00', 'producer': 'pdfTeX-1.40.25', 'source': 'Data/deepseek-v3.pdf', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2025-02-19T02:11:22+00:00', 'subject': '', 'page': 31, 'title': '', 'author': ''}, page_content='in areas such as software engineering and algorithm development, empowering developers\\nand researchers to push the boundaries of what open-source models can achieve in coding\\ntasks. In algorithmic tasks, DeepSeek-V3 demonstrates superior performance, outperforming\\nall baselines on benchmarks like HumanEval-Mul and LiveCodeBench. This success can be\\nattributed to its advanced knowledge distillation technique, which effectively enhances its code\\ngeneration and problem-solving capabilities in algorithm-focused tasks.\\nOn math benchmarks, DeepSeek-V3 demonstrates exceptional performance, significantly\\nsurpassing baselines and setting a new state-of-the-art for non-o1-like models. Specifically, on\\nAIME, MATH-500, and CNMO 2024, DeepSeek-V3 outperforms the second-best model, Qwen2.5\\n72B, by approximately 10% in absolute scores, which is a substantial margin for such challenging\\nbenchmarks. This remarkable capability highlights the effectiveness of the distillation technique\\nfrom DeepSeek-R1, which has been proven highly beneficial for non-o1-like models.\\nChinese Benchmarks. Qwen and DeepSeek are two representative model series with robust\\nsupport for both Chinese and English. On the factual benchmark Chinese SimpleQA, DeepSeek-\\nV3 surpasses Qwen2.5-72B by 16.4 points, despite Qwen2.5 being trained on a larger corpus\\ncompromising 18T tokens, which are 20% more than the 14.8T tokens that DeepSeek-V3 is\\n32'),\n",
       " Document(id='c4b054b6-f8c2-4396-ac38-b0ef6ef7ddb4', metadata={'moddate': '2025-02-19T02:11:22+00:00', 'author': '', 'subject': '', 'producer': 'pdfTeX-1.40.25', 'page': 5, 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-19T02:11:22+00:00', 'source': 'Data/deepseek-v3.pdf', 'title': '', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'page_label': '6', 'total_pages': 53, 'keywords': '', 'trapped': '/False'}, page_content='verification and reflection patterns of R1 into DeepSeek-V3 and notably improves its\\nreasoning performance. Meanwhile, we also maintain control over the output style and\\nlength of DeepSeek-V3.\\nSummary of Core Evaluation Results\\n• Knowledge: (1) On educational benchmarks such as MMLU, MMLU-Pro, and GPQA,\\nDeepSeek-V3 outperforms all other open-source models, achieving 88.5 on MMLU, 75.9\\non MMLU-Pro, and 59.1 on GPQA. Its performance is comparable to leading closed-source\\nmodels like GPT-4o and Claude-Sonnet-3.5, narrowing the gap between open-source\\nand closed-source models in this domain. (2) For factuality benchmarks, DeepSeek-V3\\ndemonstrates superior performance among open-source models on both SimpleQA and\\nChinese SimpleQA. While it trails behind GPT-4o and Claude-Sonnet-3.5 in English factual\\nknowledge (SimpleQA), it surpasses these models in Chinese factual knowledge (Chinese\\nSimpleQA), highlighting its strength in Chinese factual knowledge.\\n• Code, Math, and Reasoning: (1) DeepSeek-V3 achieves state-of-the-art performance on\\nmath-related benchmarks among all non-long-CoT open-source and closed-source models.\\nNotably, it even outperforms o1-preview on specific benchmarks, such as MATH-500,\\ndemonstrating its robust mathematical reasoning capabilities. (2) On coding-related tasks,\\nDeepSeek-V3 emerges as the top-performing model for coding competition benchmarks,\\nsuch as LiveCodeBench, solidifying its position as the leading model in this domain. For'),\n",
       " Document(id='c88678b4-fa8e-4b88-b20d-7f4cd35b874c', metadata={'subject': '', 'title': '', 'creationdate': '2025-02-19T02:11:22+00:00', 'page': 5, 'source': 'Data/deepseek-v3.pdf', 'keywords': '', 'moddate': '2025-02-19T02:11:22+00:00', 'trapped': '/False', 'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'author': '', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'page_label': '6', 'total_pages': 53}, page_content='DeepSeek-V3 emerges as the top-performing model for coding competition benchmarks,\\nsuch as LiveCodeBench, solidifying its position as the leading model in this domain. For\\nengineering-related tasks, while DeepSeek-V3 performs slightly below Claude-Sonnet-3.5,\\nit still outpaces all other models by a significant margin, demonstrating its competitiveness\\nacross diverse technical benchmarks.\\nIn the remainder of this paper, we first present a detailed exposition of our DeepSeek-V3\\nmodel architecture (Section 2). Subsequently, we introduce our infrastructures, encompassing\\nour compute clusters, the training framework, the support for FP8 training, the inference\\ndeployment strategy, and our suggestions on future hardware design. Next, we describe our\\npre-training process, including the construction of training data, hyper-parameter settings, long-\\ncontext extension techniques, the associated evaluations, as well as some discussions (Section 4).\\nThereafter, we discuss our efforts on post-training, which include Supervised Fine-Tuning (SFT),\\nReinforcement Learning (RL), the corresponding evaluations, and discussions (Section 5). Lastly,\\nwe conclude this work, discuss existing limitations of DeepSeek-V3, and propose potential\\ndirections for future research (Section 6).\\n2. Architecture\\nWe first introduce the basic architecture of DeepSeek-V3, featured by Multi-head Latent Atten-\\ntion (MLA) (DeepSeek-AI, 2024c) for efficient inference and DeepSeekMoE (Dai et al., 2024)'),\n",
       " Document(id='89061abd-f233-4845-845a-19679fa4bc0d', metadata={'page_label': '35', 'producer': 'pdfTeX-1.40.25', 'page': 34, 'moddate': '2025-02-19T02:11:22+00:00', 'title': '', 'creator': 'LaTeX with hyperref', 'total_pages': 53, 'keywords': '', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'trapped': '/False', 'creationdate': '2025-02-19T02:11:22+00:00', 'source': 'Data/deepseek-v3.pdf', 'author': ''}, page_content='DeepSeek-V3 has some limitations, especially on the deployment. Firstly, to ensure efficient\\ninference, the recommended deployment unit for DeepSeek-V3 is relatively large, which might\\npose a burden for small-sized teams. Secondly, although our deployment strategy for DeepSeek-\\nV3 has achieved an end-to-end generation speed of more than two times that of DeepSeek-V2,\\nthere still remains potential for further enhancement. Fortunately, these limitations are expected\\nto be naturally addressed with the development of more advanced hardware.\\nDeepSeek consistently adheres to the route of open-source models with longtermism, aiming\\nto steadily approach the ultimate goal of AGI (Artificial General Intelligence). In the future, we\\nplan to strategically invest in research across the following directions.\\n• We will consistently study and refine our model architectures, aiming to further improve\\nboth the training and inference efficiency, striving to approach efficient support for infinite\\ncontext length. Additionally, we will try to break through the architectural limitations of\\nTransformer, thereby pushing the boundaries of its modeling capabilities.\\n35')]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrived_documents = vectordb.similarity_search(query=question,k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294b130b",
   "metadata": {},
   "source": [
    "# Create a Prompt by Agumenting the retrieved documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d6ef0e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "Context:\n",
      "\n",
      "demonstrates the strong capability of DeepSeek-V3 in handling extremely long-context tasks.\n",
      "The long-context capability of DeepSeek-V3 is further validated by its best-in-class performance\n",
      "on LongBench v2, a dataset that was released just a few weeks before the launch of DeepSeek\n",
      "V3. On the factual knowledge benchmark, SimpleQA, DeepSeek-V3 falls behind GPT-4o and\n",
      "Claude-Sonnet, primarily due to its design focus and resource allocation. DeepSeek-V3 assigns\n",
      "more training tokens to learn Chinese knowledge, leading to exceptional performance on the\n",
      "C-SimpleQA. On the instruction-following benchmark, DeepSeek-V3 significantly outperforms\n",
      "its predecessor, DeepSeek-V2-series, highlighting its improved ability to understand and adhere\n",
      "to user-defined format constraints.\n",
      "Code and Math Benchmarks. Coding is a challenging and practical task for LLMs, encom-\n",
      "passing engineering-focused tasks like SWE-Bench-Verified and Aider, as well as algorithmic\n",
      "tasks such as HumanEval and LiveCodeBench. In engineering tasks, DeepSeek-V3 trails behind\n",
      "Claude-Sonnet-3.5-1022 but significantly outperforms open-source models. The open-source\n",
      "DeepSeek-V3 is expected to foster advancements in coding-related engineering tasks. By pro-\n",
      "viding access to its robust capabilities, DeepSeek-V3 can drive innovation and improvement\n",
      "in areas such as software engineering and algorithm development, empowering developers\n",
      "and researchers to push the boundaries of what open-source models can achieve in coding\n",
      "\n",
      "in areas such as software engineering and algorithm development, empowering developers\n",
      "and researchers to push the boundaries of what open-source models can achieve in coding\n",
      "tasks. In algorithmic tasks, DeepSeek-V3 demonstrates superior performance, outperforming\n",
      "all baselines on benchmarks like HumanEval-Mul and LiveCodeBench. This success can be\n",
      "attributed to its advanced knowledge distillation technique, which effectively enhances its code\n",
      "generation and problem-solving capabilities in algorithm-focused tasks.\n",
      "On math benchmarks, DeepSeek-V3 demonstrates exceptional performance, significantly\n",
      "surpassing baselines and setting a new state-of-the-art for non-o1-like models. Specifically, on\n",
      "AIME, MATH-500, and CNMO 2024, DeepSeek-V3 outperforms the second-best model, Qwen2.5\n",
      "72B, by approximately 10% in absolute scores, which is a substantial margin for such challenging\n",
      "benchmarks. This remarkable capability highlights the effectiveness of the distillation technique\n",
      "from DeepSeek-R1, which has been proven highly beneficial for non-o1-like models.\n",
      "Chinese Benchmarks. Qwen and DeepSeek are two representative model series with robust\n",
      "support for both Chinese and English. On the factual benchmark Chinese SimpleQA, DeepSeek-\n",
      "V3 surpasses Qwen2.5-72B by 16.4 points, despite Qwen2.5 being trained on a larger corpus\n",
      "compromising 18T tokens, which are 20% more than the 14.8T tokens that DeepSeek-V3 is\n",
      "32\n",
      "\n",
      "verification and reflection patterns of R1 into DeepSeek-V3 and notably improves its\n",
      "reasoning performance. Meanwhile, we also maintain control over the output style and\n",
      "length of DeepSeek-V3.\n",
      "Summary of Core Evaluation Results\n",
      "• Knowledge: (1) On educational benchmarks such as MMLU, MMLU-Pro, and GPQA,\n",
      "DeepSeek-V3 outperforms all other open-source models, achieving 88.5 on MMLU, 75.9\n",
      "on MMLU-Pro, and 59.1 on GPQA. Its performance is comparable to leading closed-source\n",
      "models like GPT-4o and Claude-Sonnet-3.5, narrowing the gap between open-source\n",
      "and closed-source models in this domain. (2) For factuality benchmarks, DeepSeek-V3\n",
      "demonstrates superior performance among open-source models on both SimpleQA and\n",
      "Chinese SimpleQA. While it trails behind GPT-4o and Claude-Sonnet-3.5 in English factual\n",
      "knowledge (SimpleQA), it surpasses these models in Chinese factual knowledge (Chinese\n",
      "SimpleQA), highlighting its strength in Chinese factual knowledge.\n",
      "• Code, Math, and Reasoning: (1) DeepSeek-V3 achieves state-of-the-art performance on\n",
      "math-related benchmarks among all non-long-CoT open-source and closed-source models.\n",
      "Notably, it even outperforms o1-preview on specific benchmarks, such as MATH-500,\n",
      "demonstrating its robust mathematical reasoning capabilities. (2) On coding-related tasks,\n",
      "DeepSeek-V3 emerges as the top-performing model for coding competition benchmarks,\n",
      "such as LiveCodeBench, solidifying its position as the leading model in this domain. For\n",
      "\n",
      "DeepSeek-V3 emerges as the top-performing model for coding competition benchmarks,\n",
      "such as LiveCodeBench, solidifying its position as the leading model in this domain. For\n",
      "engineering-related tasks, while DeepSeek-V3 performs slightly below Claude-Sonnet-3.5,\n",
      "it still outpaces all other models by a significant margin, demonstrating its competitiveness\n",
      "across diverse technical benchmarks.\n",
      "In the remainder of this paper, we first present a detailed exposition of our DeepSeek-V3\n",
      "model architecture (Section 2). Subsequently, we introduce our infrastructures, encompassing\n",
      "our compute clusters, the training framework, the support for FP8 training, the inference\n",
      "deployment strategy, and our suggestions on future hardware design. Next, we describe our\n",
      "pre-training process, including the construction of training data, hyper-parameter settings, long-\n",
      "context extension techniques, the associated evaluations, as well as some discussions (Section 4).\n",
      "Thereafter, we discuss our efforts on post-training, which include Supervised Fine-Tuning (SFT),\n",
      "Reinforcement Learning (RL), the corresponding evaluations, and discussions (Section 5). Lastly,\n",
      "we conclude this work, discuss existing limitations of DeepSeek-V3, and propose potential\n",
      "directions for future research (Section 6).\n",
      "2. Architecture\n",
      "We first introduce the basic architecture of DeepSeek-V3, featured by Multi-head Latent Atten-\n",
      "tion (MLA) (DeepSeek-AI, 2024c) for efficient inference and DeepSeekMoE (Dai et al., 2024)\n",
      "\n",
      "DeepSeek-V3 has some limitations, especially on the deployment. Firstly, to ensure efficient\n",
      "inference, the recommended deployment unit for DeepSeek-V3 is relatively large, which might\n",
      "pose a burden for small-sized teams. Secondly, although our deployment strategy for DeepSeek-\n",
      "V3 has achieved an end-to-end generation speed of more than two times that of DeepSeek-V2,\n",
      "there still remains potential for further enhancement. Fortunately, these limitations are expected\n",
      "to be naturally addressed with the development of more advanced hardware.\n",
      "DeepSeek consistently adheres to the route of open-source models with longtermism, aiming\n",
      "to steadily approach the ultimate goal of AGI (Artificial General Intelligence). In the future, we\n",
      "plan to strategically invest in research across the following directions.\n",
      "• We will consistently study and refine our model architectures, aiming to further improve\n",
      "both the training and inference efficiency, striving to approach efficient support for infinite\n",
      "context length. Additionally, we will try to break through the architectural limitations of\n",
      "Transformer, thereby pushing the boundaries of its modeling capabilities.\n",
      "35 \n",
      "\n",
      "Question: What is the core break-through in deep-seek v3?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "context = \"Context:\\n\\n\" + \"\\n\\n\".join([doc.page_content for doc in retrived_documents])\n",
    "qa_prompt = f\"\"\"\n",
    "Use the following pieces of context to answer the user's question. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "----------------\n",
    "{context} \n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "print(qa_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7004d72b",
   "metadata": {},
   "source": [
    "# Generation Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c3cee8ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The core breakthrough in DeepSeek-V3 is its strong capability to handle extremely long-context tasks, demonstrated by its best-in-class performance on LongBench v2 and its ability to process and reason over very lengthy inputs effectively. This long-context handling is further validated by its superior performance on various benchmarks, including long-context and reasoning tasks, highlighting its advanced architecture and training techniques designed to extend context length and improve reasoning over extended sequences.\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(qa_prompt).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b05c39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
